# 神经网络

### 概念

**定义**：神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所做出的交互反应。

在生物神经网络中，每个神经元与其他神经元相连，当它兴奋时，就会向相连的神经元发送化学物质，从而改变这些神经元内的电位；如果某神经元的电位超过了一个阈值，那么它就会激活，即兴奋起来并向其他神经元发送化学物质。

在深度学习中也借鉴了这样的结构，每一个神经元（上面说到的简单单元）接受输入 x，通过带权重 w 的连接进行传递，将总输入信号与神经元的阈值进行比较，最后通过激活函数处理确定是否激活，并将激活后的计算结果 y 输出，而我们所说的训练，所训练的就是这里面的权重 w。



### 神经网络的表示

我们可以将神经元拼接起来，两层神经元，即输入层+输出层( M-P 神经元)，构成感知机。 而多层功能神经元相连构成神经网络，输入层与输出层之间的所有层神经元，称为隐藏层： 

![img](https://handbook.pytorch.wiki/chapter2/7.png) 

如上图所示，输入层和输出层只有一个，中间的隐藏层可以有很多层



### 激活函数

神经元会对化学物质的刺激进行，当达到一定程度的时候，神经元才会兴奋，并向其他神经元发送信息。神经网络中的激活函数就是用来判断我们所计算的信息是否达到了往后面传输的条件。

在神经网络的计算过程中，每层都相当于矩阵相乘，无论神经网络有多少层输出都是输入的线性组合，就算我们有几千层的计算，无非还是个矩阵相乘，和一层矩阵相乘所获得的信息差距不大，所以需要激活函数来引入非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中，增加了神经网络模型泛化的特性。

早期研究神经网络主要采用 sigmoid 函数或者 tanh 函数，输出有界，很容易充当下一层的输入。 近些年 Relu 函数及其改进型（如 Leaky-ReLU、P-ReLU、R-ReLU 等），由于计算简单、效果好所以在多层神经网络中应用比较多。

#### ReLU 函数

Relu（Rectified Linear Units）修正线性单元

$a=max(0,z)$ 导数大于 0 时 1，小于 0 时 0。

也就是说： z > 0 时，梯度始终为 1，从而提高神经网络基于梯度算法的运算速度。然而当 z < 0 时，梯度一直为 0。 ReLU 函数只有线性关系（只需要判断输入是否大于 0）不管是前向传播还是反向传播，都比 sigmod 和 tanh 要快很多

![png](https://handbook.pytorch.wiki/chapter2/2.3-deep-learning-neural-network-introduction_files/2.3-deep-learning-neural-network-introduction_13_1.png)

当输入是负数的时候，ReLU 是完全不被激活的，这就表明一旦输入到了负数，ReLU 就会死掉。但是到了反向传播过程中，输入负数，梯度就会完全到0，这个和 sigmod 函数、tanh 函数有一样的问题。 但是实际的运用中，该缺陷的影响不是很大。

#### Leaky Relu 函数

为了解决 relu 函数 z < 0 时的问题出现了 Leaky ReLU 函数，该函数保证在 z < 0 的时候，梯度仍然不为 0。 ReLU 的前半段设为 αz 而非 0，通常 α=0.01 $ a=max(\alpha z,z)$

![png](https://handbook.pytorch.wiki/chapter2/2.3-deep-learning-neural-network-introduction_files/2.3-deep-learning-neural-network-introduction_16_1.png)

理论上来讲，Leaky ReLU 有 ReLU 的所有优点，但是在实际操作当中，并没有完全证明 Leaky ReLU 总是好于 ReLU。

ReLU 目前仍是最常用的 activation function，在隐藏层中推荐优先尝试！



### 前向传播和反向传播

![img](https://handbook.pytorch.wiki/chapter2/8.png)

#### 前向传播

对于一个神经网络来说，把输入特征 $a^{[0]}$ 这个输入值就是我们的输入 $x$，放入第一层并计算第一层的激活函数，用 $a^{[1]}$ 表示，本层中训练的结果用 $W^{[1]}$ 和 $b^{[l]}$ 来表示，这两个值与计算的结果 $z^{[1]}$ 值都需要进行缓存，而计算的结果还需要通过激活函数生成激活后的 $a^{[1]}$，即第一层的输出值，这个值会作为第二层的输入传到第二层，第二层里，需要用到 $W^{[2]}$ 和 $b^{[2]}$，计算结果为 $z^{[2]}$，第二层的激活函数 $a^{[2]}$。 后面几层以此类推，直到最后算出了 $a^{[L]}$，第 $L$ 层的最终输出值 $\hat{y}$，即我们网络的预测值。正向传播其实就是我们的输入 $x$ 通过一系列的网络计算，得到 $\hat{y}$ 的过程。

在这个过程里我们缓存的值，会在后面的反向传播中用到。

#### 反向传播

对反向传播的步骤而言，就是对正向传播的一系列的反向迭代，通过反向计算梯度，来优化我们需要训练的 $W$ 和 $b$。 把 ${\delta}a^{[l]}$ 值进行求导得到 ${\delta}a^{[l-1]}$，以此类推，直到我们得到 ${\delta}a^{[2]}$ 和 ${\delta}a^{[1]}$。反向传播步骤中也会输出 ${\delta}W^{[l]}$ 和 ${\delta}b^{[l]}$。这一步我们已经得到了权重的变化量，下面我们要通过学习率来对训练的 $W$ 和 $b$ 进行更新，

$W=W-\alpha{\delta}W $

$b=b-\alpha{\delta}b $

这样反向传播就就算是完成了



### 小结

简单来说，神经网络就是构造了一个从输入层 X 到 输出层  Y 的一系列运算，最终的目的是输出值与预期值相吻合。

![image-20220510113130741](img\神经网络示意图.png)

中间每一层（以第 $l$ 层为例）计算，包括了

$a^{(l)} = \sigma(z^{(l)}) $

$z^{(l)} = \omega^{(l)}a^{(l-1)} + b^{(l)}$

其中，$\omega$ 为权重，$b$ 为偏移量，$z$ 为线性运算的结果，$a$ 为这一层的计算结果，$\sigma$ 是激活函数，左上角表示当前的变量都是第 $l$ 层

经过各层的运算，得到最终输出结果 $a^{(n)}$，将这个结果与预期输出 y 作比较

$L = \frac12||a^{(n)}-y||$ 

我们的最终目的是要是 L 尽可能小，所以接下来会反向迭代神经网络，**修改 $\omega$ 和 $b$ ** ，修改的方法有很多，比如最速下降法，通过求梯度完成：

![image-20220510112439383](img\最速下降法.png)

当然，以上过程，只是一个样本的一次训练，当引入大量样本并进行多次训练后，就可以得到我们想要的神经网络，或者更准确一点说，得到想要的 $\omega$ 和 $b$



# 卷积神经网络 CNN

传统的网络需要大量的参数，但是这些参数是否重复了呢，例如，我们识别一个人，只要看到他的眼睛，鼻子，嘴，还有脸基本上就知道这个人是谁了，只是用这些局部的特征就能做做判断了，并不需要所有的特征。 另外一点就是我们上面说的可以有效提取了输入图像的平移不变特征，就好像我们看到了这是个眼睛，这个眼镜在左边还是在右边他都是眼睛，这就是平移不变性。

 我们通过卷积的计算操作来提取图像局部的特征，每一层都会计算出一些局部特征，这些局部特征再汇总到下一层，这样一层一层的传递下去，特征由小变大，最后在通过这些局部的特征对图片进行处理，这样大大提高了计算效率，也提高了准确度。



### 结构组成

#### 卷积计算

![img](https://handbook.pytorch.wiki/chapter2/9.gif)

我们会定义一个权重矩阵，也就是我们说的 W（一般对于卷积来说，称作卷积的核kernel 也有有人称做过滤器 filter），这个权重矩阵的大小一般为`3 x 3` 或者`5 x 5`

我们以图上所示的方式，我们在输入矩阵上使用我们的权重矩阵进行滑动，每滑动一步，将所覆盖的值与矩阵对应的值相乘，并将结果求和并作为输出矩阵的一项，依次类推直到全部计算完成。

上图所示，我们输入是一个 `5 x 5`的矩阵，通过使用一次`3 x 3`的卷积核计算得到的计算结果是一个`3 x 3`的新矩阵。

#### 卷积核大小 f

刚才已经说到了一个重要的参数，就是核的大小，我们这里用 f 来表示

#### 边界填充 (p)adding

我们看到上图，经过计算后矩阵的大小改变了，如果要使矩阵大小不改变呢，我们可以先对矩阵做一个填充，将矩阵的周围全部再包围一层，这个矩阵就变成了`7 x 7`,上下左右各加 1，相当于 `5+1+1=7` 这时，计算的结果还是 `5 x 5`的矩阵，保证了大小不变，这里的 p = 1

#### 步长 (s)tride

从动图上我们能够看到，每次滑动只是滑动了一个距离，如果每次滑动两个距离呢？那就需要使用步长这个参数。

#### 计算公式

n 为我们输入的矩阵的大小，$ \frac{n-f+2p}{s} +1 $ 向下取整

这个公式非常重要一定要记住

#### 卷积层

在每一个卷积层中我们都会设置多个核，每个核代表着不同的特征，这些特征就是我们需要传递到下一层的输出，而我们训练的过程就是训练这些不同的核。

#### 池化层（pooling）

池化层是 CNN 的重要组成部分，通过减少卷积层之间的连接，降低运算复杂程度，池化层的操作很简单，就想相当于是合并，我们输入一个过滤器的大小，与卷积的操作一样，也是一步一步滑动，但是过滤器覆盖的区域进行合并，只保留一个值。 合并的方式也有很多种，例如我们常用的两种取最大值 maxpooling，取平均值 avgpooling

池化层的输出大小公式也与卷积层一样，由于没有进行填充，所以 p = 0，可以简化为 $ \frac{n-f}{s} +1 $

#### dropout 层

dropout 是 Hinton 提出防止过拟合而采用的 trick，增强了模型的泛化能力 Dropout（随机失活）。是指在深度学习网络的训练过程中，按照一定的概率将一部分神经网络单元暂时从网络中丢弃，相当于从原始的网络中找到一个更瘦的网络，说的通俗一点，就是随机将一部分网络的传播掐断，听起来好像不靠谱，但是通过实际测试效果非常好。 

#### 全连接层

全链接层一般是作为最后的输出层使用，卷积的作用是提取图像的特征，最后的全连接层就是要通过这些特征来进行计算，输出我们所要的结果了，无论是分类，还是回归。

我们的特征都是使用矩阵表示的，所以再传入全连接层之前还需要对特征进行压扁，将他这些特征变成一维的向量，如果要进行分类的话，就是用 sofmax 作为输出，如果要是回归的话就直接使用 linear 即可。



