# 聊天机器人大作业实验报告

## 1. GUI

### 1.1 开发环境

- python 3.8.10

- pyqt5 5.15.4

- pyqt5-plugins 5.15.4.2.2

- pyqt5-sip 12.10.1

- pyqt5-tools 5.15.4.3.2

- pyqtwebengine	5.15.5

- pyqtwebengine-qt5 5.15.2

### 1.2 技术路径

### 1.3 运行逻辑

### 1.4 实现过程



## 3. 神经网络

### 3.1 开发环境

+ python 3.8.10
+ nltk 3.7
+ numpy 1.22.3
+ torch 1.11.0

### 3.2 理论依据

#### 3.2.1 前馈神经网络 FNN

##### 神经网络的概念

**定义**：神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所做出的交互反应。

在生物神经网络中，每个神经元与其他神经元相连，当它兴奋时，就会向相连的神经元发送化学物质，从而改变这些神经元内的电位；如果某神经元的电位超过了一个阈值，那么它就会激活，即兴奋起来并向其他神经元发送化学物质。

在深度学习中也借鉴了这样的结构，每一个神经元（上面说到的简单单元）接受输入 x，通过带权重 w 的连接进行传递，将总输入信号与神经元的阈值进行比较，最后通过激活函数处理确定是否激活，并将激活后的计算结果 y 输出，而我们所说的训练，所训练的就是这里面的权重 w。

##### 神经网络的表示

我们可以将神经元拼接起来，两层神经元，即输入层+输出层( M-P 神经元)，构成感知机。 而多层功能神经元相连构成神经网络，输入层与输出层之间的所有层神经元，称为隐藏层： 

![img](https://handbook.pytorch.wiki/chapter2/7.png) 

如上图所示，输入层和输出层只有一个，中间的隐藏层可以有很多层

#### 3.2.2 激活函数

神经元会对化学物质的刺激进行，当达到一定程度的时候，神经元才会兴奋，并向其他神经元发送信息。神经网络中的激活函数就是用来判断我们所计算的信息是否达到了往后面传输的条件。

在神经网络的计算过程中，每层都相当于矩阵相乘，无论神经网络有多少层输出都是输入的线性组合，就算我们有几千层的计算，无非还是个矩阵相乘，和一层矩阵相乘所获得的信息差距不大，所以需要激活函数来引入非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中，增加了神经网络模型泛化的特性。

早期研究神经网络主要采用 sigmoid 函数或者 tanh 函数，输出有界，很容易充当下一层的输入。 近些年 Relu 函数及其改进型（如 Leaky-ReLU、P-ReLU、R-ReLU 等），由于计算简单、效果好所以在多层神经网络中应用比较多。

##### ReLU 函数

Relu（Rectified Linear Units）修正线性单元

$a=max(0,z)$ 导数大于 0 时 1，小于 0 时 0。

也就是说： z > 0 时，梯度始终为 1，从而提高神经网络基于梯度算法的运算速度。然而当 z < 0 时，梯度一直为 0。 ReLU 函数只有线性关系（只需要判断输入是否大于 0）不管是前向传播还是反向传播，都比 sigmod 和 tanh 要快很多

![png](https://handbook.pytorch.wiki/chapter2/2.3-deep-learning-neural-network-introduction_files/2.3-deep-learning-neural-network-introduction_13_1.png)

当输入是负数的时候，ReLU 是完全不被激活的，这就表明一旦输入到了负数，ReLU 就会死掉。但是到了反向传播过程中，输入负数，梯度就会完全到0，这个和 sigmod 函数、tanh 函数有一样的问题。 但是实际的运用中，该缺陷的影响不是很大。

##### Leaky Relu 函数

为了解决 relu 函数 z < 0 时的问题出现了 Leaky ReLU 函数，该函数保证在 z < 0 的时候，梯度仍然不为 0。 ReLU 的前半段设为 αz 而非 0，通常 α=0.01 $ a=max(\alpha z,z)$

![png](https://handbook.pytorch.wiki/chapter2/2.3-deep-learning-neural-network-introduction_files/2.3-deep-learning-neural-network-introduction_16_1.png)

理论上来讲，Leaky ReLU 有 ReLU 的所有优点，但是在实际操作当中，并没有完全证明 Leaky ReLU 总是好于 ReLU。

#### 3.2.3 前向传播和反向传播

![img](https://handbook.pytorch.wiki/chapter2/8.png)

##### 前向传播

对于一个神经网络来说，把输入特征 $a^{[0]}$ 这个输入值就是我们的输入 $x$，放入第一层并计算第一层的激活函数，用 $a^{[1]}$ 表示，本层中训练的结果用 $W^{[1]}$ 和 $b^{[l]}$ 来表示，这两个值与计算的结果 $z^{[1]}$ 值都需要进行缓存，而计算的结果还需要通过激活函数生成激活后的 $a^{[1]}$，即第一层的输出值，这个值会作为第二层的输入传到第二层，第二层里，需要用到 $W^{[2]}$ 和 $b^{[2]}$，计算结果为 $z^{[2]}$，第二层的激活函数 $a^{[2]}$。 后面几层以此类推，直到最后算出了 $a^{[L]}$，第 $L$ 层的最终输出值 $\hat{y}$，即我们网络的预测值。正向传播其实就是我们的输入 $x$ 通过一系列的网络计算，得到 $\hat{y}$ 的过程。

在这个过程里我们缓存的值，会在后面的反向传播中用到。

##### 反向传播

对反向传播的步骤而言，就是对正向传播的一系列的反向迭代，通过反向计算梯度，来优化我们需要训练的 $W$ 和 $b$。 把 ${\delta}a^{[l]}$ 值进行求导得到 ${\delta}a^{[l-1]}$，以此类推，直到我们得到 ${\delta}a^{[2]}$ 和 ${\delta}a^{[1]}$。反向传播步骤中也会输出 ${\delta}W^{[l]}$ 和 ${\delta}b^{[l]}$。这一步我们已经得到了权重的变化量，下面我们要通过学习率来对训练的 $W$ 和 $b$ 进行更新，

$W=W-\alpha{\delta}W $

$b=b-\alpha{\delta}b $

这样反向传播就就算是完成了



#### 3.2.4 FNN 小结

简单来说，神经网络就是构造了一个从输入层 X 到 输出层  Y 的一系列运算，最终的目的是输出值与预期值相吻合。

![image-20220510113130741](C:\Users\86198\AppData\Roaming\Typora\typora-user-images\image-20220510113130741.png)

中间每一层（以第 $l$ 层为例）计算，包括了

$a^{(l)} = \sigma(z^{(l)}) $

$z^{(l)} = \omega^{(l)}a^{(l-1)} + b^{(l)}$

其中，$\omega$ 为权重，$b$ 为偏移量，$z$ 为线性运算的结果，$a$ 为这一层的计算结果，$\sigma$ 是激活函数，左上角表示当前的变量都是第 $l$ 层

经过各层的运算，得到最终输出结果 $a^{(n)}$，将这个结果与预期输出 y 作比较

$L = \frac12||a^{(n)}-y||$ 

我们的最终目的是要是 L 尽可能小，所以接下来会反向迭代神经网络，**修改 $\omega$ 和 $b$ ** ，修改的方法有很多，比如最速下降法，通过求梯度完成：

![image-20220510112439383](C:\Users\86198\AppData\Roaming\Typora\typora-user-images\image-20220510112439383.png)

当然，以上过程，只是一个样本的一次训练，当引入大量样本并进行多次训练后，就可以得到我们想要的神经网络，或者更准确一点说，得到想要的 $\omega$ 和 $b$

#### 3.2.5 监督学习和无监督学习

监督学习、无监督学习、半监督学习、强化学习是我们日常接触到的常见的四个机器学习方法：

- 监督学习：通过已有的训练样本（即已知数据以及其对应的输出）去训练得到一个最优模型（这个模型属于某个函数的集合，最优则表示在某个评价准则下是最佳的），再利用这个模型将所有的输入映射为相应的输出。
- 无监督学习：它与监督学习的不同之处，在于我们事先没有任何训练样本，而需要直接对数据进行建模。
- 半监督学习 ：在训练阶段结合了大量未标记的数据和少量标签数据。与使用所有标签数据的模型相比，使用训练集的训练模型在训练时可以更为准确。
- 强化学习：我们设定一个回报函数（reward function），通过这个函数来确认否越来越接近目标，类似我们训练宠物，如果做对了就给他奖励，做错了就给予惩罚，最后来达到我们的训练目的。

#### 3.2.6 张量 Tensor

张量的英文是 Tensor，它是 PyTorch 里面基础的运算单位，与 Numpy 的 ndarray 相同，都表示一个**多维的矩阵**。

PyTorch 的 Tensor 可以在 GPU 上运行，这大大加快了运算速度。

> 张量（Tensor）是一个定义在一些向量空间和一些对偶空间的笛卡儿积上的多重线性映射，其坐标是|n|维空间内，有|n|个分量的一种量， 其中每个分量都是坐标的函数， 而在坐标变换时，这些分量也依照某些规则作线性变换。r 称为该张量的秩或阶（与矩阵的秩和阶均无关系）。 (来自[百度百科](https://baike.baidu.com/item/张量/380114?fr=aladdin))

#### 3.2.7 线性回归 Linear Regreesion

线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。其表达形式为 `y = w'x+e`，e 为误差服从均值为 0 的正态分布。

> 回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。

简单的说： 线性回归对于输入 x 与输出 y 有一个映射 f，$y=f(x)$, 而 f 的形式为 $ax+b$。其中 a 和 b 是两个可调的参数，我们训练的时候就是训练 a，b 这两个参数

#### 3.3.8 损失函数 Lose Function

损失函数（loss function）是用来估量模型的预测值与真实值的不一致程度，它是一个非负实值函数，损失函数越小，模型的鲁棒性就越好。 我们训练模型的过程，就是通过不断的迭代计算，使用梯度下降的优化算法，使得损失函数越来越小。损失函数越小就表示算法达到意义上的最优。

##### nn.L1Loss

输入 x 和目标 y 之间差的绝对值，要求 x 和 y 的维度要一样（可以是向量或者矩阵），得到的 loss 维度也是对应一样的

$ loss(x,y)=1/n\sum|x_i-y_i| $

##### nn.NLLLoss

用于多分类的负对数似然损失函数

$ loss(x, class) = -x[class]$

NLLLoss 中如果传递了 weights 参数，会对损失进行加权，公式就变成了

$ loss(x, class) = -weights[class] * x[class] $

##### nn.MSELoss

均方损失函数 ，输入 x 和目标 y 之间均方差

$ loss(x,y)=1/n\sum(x_i-y_i)^2 $

##### nn.CrossEntropyLoss

多分类用的交叉熵损失函数，LogSoftMax 和 NLLLoss 集成到一个类中，会调用 nn.NLLLoss 函数，我们可以理解为 CrossEntropyLoss() = log_softmax() + NLLLoss()

$ \begin{aligned} loss(x, class) &= -\text{log}\frac{exp(x[class])}{\sum_j exp(x[j]))}\ &= -x[class] + log(\sum_j exp(x[j])) \end{aligned} $

因为使用了 NLLLoss，所以也可以传入 weight 参数，这时 loss 的计算公式变为：

$ loss(x, class) = weights[class] * (-x[class] + log(\sum_j exp(x[j]))) $

所以一般多分类的情况会使用这个损失函数

##### nn.BCELoss

计算 x 与 y 之间的二进制交叉熵。

$ loss(o,t)=-\frac{1}{n}\sum_i(t[i] *log(o[i])+(1-t[i])* log(1-o[i])) $

与 NLLLoss 类似，也可以添加权重参数：

$ loss(o,t)=-\frac{1}{n}\sum_iweights[i] *(t[i]* log(o[i])+(1-t[i])* log(1-o[i])) $

用的时候需要在该层前面加上 Sigmoid 函数。

#### 3.3.9 梯度下降

梯度下降是一个使损失函数越来越小的优化算法，在无求解机器学习算法的模型参数，即约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一。

##### 梯度

在微积分里面，对多元函数的参数求 ∂ 偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。 例如函数 f(x,y), 分别对 x，y 求偏导数，求得的梯度向量就是 $(∂f/∂x, ∂f/∂y)^T$ ，简称 grad f(x,y) 或者 ▽f(x,y)。

几何上讲，梯度就是函数变化增加最快的地方，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向梯度减少最快，也就是更加容易找到函数的最小值。

我们需要最小化损失函数，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数，和模型参数值。

##### 梯度下降法直观解释

梯度下降法就好比下山，我们并不知道下山的路，于是决定走一步算一步，每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。这样一步步的走下去，一直走到觉得我们已经到了山脚。

如下图所示

 ![img](https://handbook.pytorch.wiki/chapter2/1.png)

这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山峰低处（局部最优解）。

这个问题在以前的机器学习中可能会遇到，因为机器学习中的特征比较少，所以导致很可能陷入到一个局部最优解中出不来，但是到了深度学习，动辄百万甚至上亿的特征，出现这种情况的概率几乎为 0，所以我们可以不用考虑这个问题。

##### Mini-batch 的梯度下降法

对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候处理速度会很慢，而且也不可能一次的载入到内存或者显存中，所以我们会把大数据集分成小数据集，一部分一部分的训练，这个训练子集即称为 Mini-batch。 在 PyTorch 中就是使用这种方法进行的训练.

##### torch.optim.SGD

随机梯度下降算法，带有动量（momentum）的算法作为一个可选参数可以进行设置，样例如下：

```python
#lr参数为学习率，对于SGD来说一般选择0.1 0.01.0.001，如何设置会在后面实战的章节中详细说明
#如果设置了momentum，就是带有动量的SGD，可以不设置
optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
```

##### torch.optim.RMSprop

除了以上的带有动量 Momentum 梯度下降法外，RMSprop（root mean square prop）也是一种可以加快梯度下降的算法，利用 RMSprop 算法，可以减小某些维度梯度更新波动较大的情况，使其梯度下降的速度变得更快

```python
optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99)
```

##### torch.optim.Adam

Adam 优化算法的基本思想就是将 Momentum 和 RMSprop 结合起来形成的一种适用于不同深度学习结构的优化算法

```python
# 这里的lr，betas，还有eps都是用默认值即可，所以Adam是一个使用起来最简单的优化方法
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)
```

#### 3.3.10 方差/偏差

- 偏差度量了学习算法的期望预测与真实结果的偏离程序，即刻画了学习算法本身的拟合能力

- 方差度量了同样大小的训练集的变动所导致的学习性能的变化，即模型的泛化能力

  ![img](https://handbook.pytorch.wiki/chapter2/5.png)

从图中我们可以看出：

高偏差（high bias）的情况，一般称为欠拟合（underfitting），即我们的模型并没有很好的去适配现有的数据，拟合度不够。 

高方差（high variance）的情况，一般称作过拟合（overfitting），即模型对于训练数据拟合度太高了，失去了泛化的能力。

如何解决这两种情况呢？

欠拟合： 增加网络结构，如增加隐藏层数目；训练更长时间；寻找合适的网络架构，使用更大的NN结构；

过拟合 ：使用更多的数据；正则化（ regularization）；寻找合适的网络结构；

#### 3.3.11 使用 PyTorch 计算梯度数值

PyTorch 的 Autograd 模块实现了深度学习的算法中的向传播求导数，在张量（Tensor类）上的所有操作，Autograd 都能为他们自动提供微分，简化了手动计算导数的复杂过程。

在张量创建时，通过设置 requires_grad 标识为 Ture 来告诉 Pytorch 需要对该张量进行自动求导，PyTorch 会记录该张量的每一步操作历史并自动计算

```python
x = torch.rand(5, 5, requires_grad=True)
y = torch.rand(5, 5, requires_grad=True)
z = torch.sum(x+y)
```

每个张量都有一个 .grad_fn 属性，如果这个张量是用户手动创建的那么这个张量的 grad_fn 是 None。

PyTorch 会自动追踪和记录对与张量的所有操作，当计算完成后调用 .backward() 方法自动计算梯度并且将计算结果保存到 grad 属性中

`x` 是手动创建的没有通过计算，所以他被认为是一个叶子节点也就是一个创建变量，而 `z` 是通过 `x` 与 `y` 的一系列计算得到的，所以不是叶子结点也就是结果变量。

##### 过程

1. 当我们执行z.backward()的时候。这个操作将调用 z 里面的 grad_fn 这个属性，执行求导的操作。
2. 这个操作将遍历 grad_fn 的 next_functions，然后分别取出里面的 Function （AccumulateGrad），执行求导操作。这部分是一个递归的过程直到最后类型为叶子节点。
3. 计算出结果以后，将结果保存到他们对应的 variable 这个变量所引用的对象（x 和 y）的  grad 这个属性里面。
4. 求导结束。所有的叶节点的 grad 变量都得到了相应的更新

最终当我们执行完 z.backward() 之后，x 和 y 里面的 grad 值就得到了更新。

#### 3.3.12 神经网络包 nn

torch.nn 是专门为神经网络设计的模块化接口，nn 构建于 Autograd 之上，可用来定义和运行神经网络。

PyTorch 中已经为我们准备好了现成的网络模型，只要继承 nn.Module，并实现它的 forward 方法，PyTorch 会根据 autograd，自动实现 backward 函数，在 forward 函数中可使用任何 tensor 支持的函数，还可以使用 if、for 循环、print、log 等 Python 语法，写法和标准的 Python 写法一致

网络的可学习参数通过net.parameters()返回

注：在反向传播前，先要将所有参数的梯度清零

#### 3.3.13 优化器

在反向传播计算完所有参数的梯度后，还需要使用优化方法来更新网络的权重和参数，例如随机梯度下降法 (SGD) 的更新策略如下：

weight = weight - learning_rate * gradient

在 torch.optim 中实现大多数的优化方法，例如 RMSProp、Adam、SGD 等



### 3.3 运行逻辑

#### 3.3.1 如何运行

将 FNN_train.py、nn_main.py 和语料库 traindata.json 放在同一路径下，运行 train.py 进行训练，再运行 main.py 即可运行

#### 3.3.2 框架

主体是一个前馈神经网络，核心功能是对输入的语句进行分类，归类到语料库中已经包含的 tag。语料库中已经写好了对各个 tag 的回复。

输入的语句会首先进行分词，根据词性决定每个词的权重，转化为一个数字向量输入进神经网络。

输出也为一个数字向量，归一化后，每一位表示语句归类到这个 tag 的概率，如果概率最大的那一项的概率超过 80%，则判定为”命中“，程序会从语料库中当前 tag 对应的回复中随机选择一条作为回复。

如果没有命中，则运行另一个神经网络，这个神经网络是引用的 PyTorch 官网的[教程](https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chatbot)

#### 3.3.3 原创内容

+ 前馈神经网络的设计、框架
+ 前馈神经网络的语料库，均为自己编写
+ 前馈神经网络输入语句的处理，即语句转化为数字向量，包括分词和根据词性赋予权重

#### 3.3.4 参考内容

+ 前馈神经网络”每句话归类为一个 tag“的匹配思想是来源于网络
+ 前馈神经网络未命中之后调用的另一个神经网络，后者来源于 PyTorch 官网教程
+ 训练好的神经网络的保存与读取



### 3.4 实现过程

#### 3.4.1 FNN_train

打开语料库，读取语句与 tag。

对语句进行处理，首先是将一句话拆分为单词，不考虑标点符号，筛选不健康的词汇进行删除，这样一句话就转换成了一个单词数组。汇总所有语句的单词构成一个数组，记其长度为 n. 对每一句话，根据它出现的单词，转换为一个长度为 n 的向量，这个向量每一位的值都对应一个语料库出现过的单词，如果当前这句话包含这个单词，则记为一个非零的数字（权值），如果不包含这个单词，则记为 0. 

而权值的具体值，则根据词性来决定，概括来讲，动词和名词的权值最高，形容词次之，介词虚词最底，这是考虑到不同词性的单词对语句含义的影响能力不同，通常动词和名词的影响能力最大。

训练的输入数据，就是很多由语句转化来的向量。训练的输出数据，是长度为 tag 的总数量的向量，向量的每个值归一化以后表示这个句子对应这个 tag 的可能值。训练的预期输出，是这个句子对应的 tag 的下标。

神经网络选用前馈神经网络，使用线性模型，三个隐藏层。激活函数选用 Relu 函数。

损失函数使用交叉熵损失函数，优化器使用 Adam.

训练预期得到的神经网络，就是一个可以将输入的一句话分类到一个 tag

#### 3.4.2 nn_main

聊天机器人的神经网络内核，输入一句话，输出一句回复。

读取两个神经网络训练好的参数。输入的一句话首先进入前馈神经网络进行分类，如果成功分类，即这句话对应一个 tag 的概率超过 80%，则视为命中，从语料库中这个 tag 对应的回复随机选取一条进行输出。

如果未命中，则将这句话输入 PyTorch 官网教程的神经网络，得到一个输出。

#### 3.4.3 traindata

自己写的语料库，每一条记录包含三部分：第一部分是输入，由多条语句构成，第二部分是这几条语句对应的 tag，第三部分是针对这个 tag 的输出语句，通常也有多条。

#### 3.4.4 其他

均为 PyTorch 官网教程给出的代码和语料库，我们直接做了引用，没有做很大的调整。





## 3. 语音输入输出

## 4. 网络查询

## 5. 参考资料

+ [Chatbot Tutorial — PyTorch Tutorials 1.11.0+cu102 documentation](https://pytorch.org/tutorials/beginner/chatbot_tutorial.html?highlight=chatbot)
+ [PyTorch 中文手册（pytorch handbook） - Pytorch中文手册](https://handbook.pytorch.wiki/index.html)